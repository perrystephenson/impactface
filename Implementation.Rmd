---
title: "Implementation"
output: github_document
---

This document will string together each of the key implementation steps and 
demonstrate the effectiveness of the end-to-end system as a static analysis. 
Some of the implementation is inside functions in the "Functions" folder of this
repository, which is being developed to allow the code to be easily re-used in
the event I get time to build a live demonstration system. These functions
should also minimise the time-to-implementation for anyone wishing to apply this
technique to a new dataset.

## Setup and Packages

```{r packages, message=FALSE}
root <- path.expand("~/impactface/")
set.seed(2016L)
library(dplyr)
library(ggplot2)
library(magrittr)
library(refimpact)
library(stringr)
library(text2vec)
library(tibble)
library(tidytext)
```

## Data Import

The data will be either imported from an RDS file on disk, or directly from the
REF Impact Case Study Database using the `refimpact` package. We will also do 
some very basic cleaning by removing superfluous whitespace from the text 
fields. 

```{r import, cache = TRUE}
source(paste0(root, "Functions/load_ref_data.R"))
ref <- load_ref_data() %>% 
  select(CaseStudyId, UOA, ImpactType, Institution, Title, ImpactDetails)
in_test <- sample(1:nrow(ref), 10)
test <- ref[ in_test, ] # Leave 10 out for testing
ref  <- ref[-in_test, ] # Keep the rest
ref$Institution %<>%   str_replace_all("\\s+", " ") %>% 
                       str_replace_all("(^\\s+)|(\\s+$)", "")
ref$Title %<>%         str_replace_all("\\s+", " ") %>% 
                       str_replace_all("(^\\s+)|(\\s+$)", "")
ref$ImpactDetails %<>% str_replace_all("\\s+", " ") %>% 
                       str_replace_all("(^\\s+)|(\\s+$)", "")
glimpse(ref)
```

If you are confused by the use of the `%<>%` operator (from the **magrittr**
package) then you should call `?'%<>%'` to learn more about it!

## Tidy Text

The basic unit of analysis in this project is the sentence, so we will need to 
break down the text into sentences for later analysis. We can do this using the
`tidytext` package.

```{r tidy-text, cache = TRUE}
tidy_ref <- unnest_tokens(ref, Sentence, ImpactDetails, "sentences", to_lower=F)
glimpse(tidy_ref)
```

## Fit a GloVe Model

In Tuning.Rmd I tried 4 different values of dimensions and 4 different values of
window length. In general, higher values of both seem to improve the ability of 
the GloVe model to score higher on the standard test set, however changing the
values of these values doesn't seem to influence the identification of outliers.
According to some irresponsibly quick reading of Google search results, the
following statements are broadly reasonable:

1. Higher-dimension models are more likely to capture meaningful relationships
1. Larger skip-gram windows are more likely to capture meaningful relationsips

However, larger skip-gram windows also mean that the model will favour 
document-level word context over sentence-level word context. Given that future
analysis in this project will be focused on sentences, I think that it makes 
sense to choose a moderate dimension (75) and a relatively small skip-window 
(5). 

```{r fit-glove, message=FALSE, cache = TRUE}
source(paste0(root, "Functions/fit_glove_model.R"))
tmp <- fit_glove_model(ref$ImpactDetails, 75, 5) 
glove <- tmp[[1]]
vocab <- tmp[[2]]
rm(tmp)
word_vectors <- glove$get_word_vectors()
```

## Locate Sentences in GloVe Representation

We will be applying a tfidf transform (and we will need to reapply this
transform later) so the tfidf model object is fit to the data prior to being
passed into the function.

```{r sentence-vectors, message=FALSE, cache = TRUE}
source(paste0(root, "Functions/fit_tfidf.R"))
tfidf <- fit_tfidf(tidy_ref$Sentence, vocab)
source(paste0(root, "Functions/get_sentence_vectors.R"))
sentence_vectors <- get_sentence_vectors(sentences = tidy_ref$Sentence, 
                                         vocab = vocab, 
                                         transform = tfidf, 
                                         wv = word_vectors)
```


## Import Unseen Data and Identify Outliers

Ten impact case studies were kept out of the training data to allow testing on
unseen data. Each of these 10 case studies will have a new sentence added to the
ImpactDetails field, and these sentences will be intentionally unsuitable for 
inclusion in an impact case study.

```{r bad-sentences, cache = TRUE}
source(paste0(root, "Functions/add_bad_sentences.R"))
test <- add_bad_sentences(test)
```

Ordinarily these unseen documents would be analysed individually, however for
the purposes of demonstration we will consider all 10 documents at once. To do
this we need to clean up the text by removing whitespace, and then break the 
documents down into individual sentences.

```{r unseen, cache = TRUE}
test$Institution %<>%   str_replace_all("\\s+", " ") %>% 
                       str_replace_all("(^\\s+)|(\\s+$)", "")
test$Title %<>%         str_replace_all("\\s+", " ") %>% 
                       str_replace_all("(^\\s+)|(\\s+$)", "")
test$ImpactDetails %<>% str_replace_all("\\s+", " ") %>% 
                       str_replace_all("(^\\s+)|(\\s+$)", "")
tidy_test <- unnest_tokens(test, Sentence, ImpactDetails, "sentences", to_lower=F)
```

We can now calculate the vector representation of each of these unseen sentences
using the pre-discovered vocabulary, the pre-trained GloVe model and the 
pre-trained tfidf transformation.

```{r unseen-vectors, cache = TRUE}
unseen_vectors <- get_sentence_vectors(sentences = tidy_test$Sentence, 
                                         vocab = vocab, 
                                         transform = tfidf, 
                                         wv = word_vectors)
```

We can now try and find those 10 "bad" sentences automatically using euclidean
distances in the GloVe model space.

### Average Sentence Approach

This approach calculates the distance from each sentence to the "average 
sentence" from the training corpus. The average sentence is quite close to zero, 
so this could also be achieved by calculating each sentence vector length in the 
GloVe vector space. 

```{r av-sentence, cache= TRUE}
average_sentence <- colMeans(as.matrix(sentence_vectors))
tidy_test$global_distance <- 
  dist2(x = as.matrix(unseen_vectors), y = t(as.matrix(average_sentence)), 
       method = "euclidean", norm = "l2")[,1]
tidy_test %>% arrange(desc(global_distance)) %>%
  select(Sentence) %>%
  head(10)
```

This approach is not working well!

### Average Distance Approach

Instead of calculating the distance from the average, we can calculate the 
average distance from the other sentences.

```{r av-distance, cache = TRUE}
distances <- 
  dist2(x = as.matrix(unseen_vectors), y = as.matrix(sentence_vectors), 
       method = "euclidean", norm = "l2")
tidy_test$average_distance <- rowMeans(distances)
rm(distances)
tidy_test %>% arrange(desc(average_distance)) %>%
  select(Sentence) %>%
  head(10)
```

This is also not ideal, and is clearly not working very well. 

### Cosine Similarity

```{r cosine-sim, cache = TRUE}
similarity <- 
  sim2(x = as.matrix(unseen_vectors), y = as.matrix(sentence_vectors), 
       method = "cosine", norm = "l2")
tidy_test$cosine_sim <- rowSums(similarity)
tidy_test %>% arrange(cosine_sim) %>%
  select(Sentence) %>%
  head(10)
```



```{r}
tidy_test$quality <- "Good"
# Get the final sentence in each case study and label it as "Bad"
case_studies <- unique(tidy_test$CaseStudyId)
rows <- c(1:10) # Dummy values
for (i in 1:10) {
  rows[i] <- max(which(tidy_test$CaseStudyId == case_studies[i]))
}
tidy_test$quality[rows] <- "Bad"
rm(rows)
# Plot some distributions
ggplot(tidy_test) + 
  geom_density(aes(x=global_distance, fill=quality), alpha=0.7)
ggplot(tidy_test) + 
  geom_density(aes(x=average_distance, fill=quality), alpha=0.7)
ggplot(tidy_test) + 
  geom_density(aes(x=cosine_sim, fill=quality), alpha=0.7)
```

These metrics are definitely useful, but they aren't perfect.

## Sentence Replacement Suggestions

The next key component is the suggestion of replacement sentences. For the 
development of this component I will be using the "known bad" sentences, i.e.
the sentences which I used to vandalise the otherwise well-written impact case
studies.

```{r}
replacement_candidates <- tidy_test$Sentence[tidy_test$quality == "Bad"]
```

We will use the Relaxed Word Mover's Distance exclusively for this part of the 
project given how well it worked in the experimentation phase.

```{r}
tokens <- 
  tidy_ref$Sentence %>% 
  str_to_lower() %>% 
  str_replace_all("[^[:alnum:]]", " ") %>% 
  str_replace_all("\\s+", " ") %>% 
  str_replace_all("(^\\s+)|(\\s+$)", "") %>% 
  word_tokenizer()
it <- itoken(tokens)
vectorizer <- vocab_vectorizer(vocab)
dtm <- create_dtm(it, vectorizer)

tokens <- 
  tidy_test$Sentence %>% 
  str_to_lower() %>% 
  str_replace_all("[^[:alnum:]]", " ") %>% 
  str_replace_all("\\s+", " ") %>% 
  str_replace_all("(^\\s+)|(\\s+$)", "") %>% 
  word_tokenizer()
it <- itoken(tokens)
vectorizer <- vocab_vectorizer(vocab)
unseen_dtm <- create_dtm(it, vectorizer)

for (i in seq_along(replacement_candidates)) {
  message("Sentence being analysed:")
  cat(replacement_candidates[i])
  
  tokens <- 
    replacement_candidates[i] %>% 
    str_to_lower() %>% 
    str_replace_all("[^[:alnum:]]", " ") %>% 
    str_replace_all("\\s+", " ") %>% 
    str_replace_all("(^\\s+)|(\\s+$)", "") %>% 
    word_tokenizer()
  it <- itoken(tokens)
  vectorizer <- vocab_vectorizer(vocab)
  unseen_dtm <- create_dtm(it, vectorizer)

  rwmd <- RelaxedWordMoversDistance$new(word_vectors)
  rwmd$verbose <- FALSE
  tidy_ref$rwmd_distance <- dist2(dtm, unseen_dtm, 
                                  method = rwmd, 
                                  norm = "none")[,1]
  suggestions <- tidy_ref %>% 
    arrange(rwmd_distance) %>% 
    head(3)
  
  message("Similar sentences from the corpus:")
  print(suggestions$Sentence)
}
```
